{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c90344a75040b88909a03816b6de48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>5</td><td>application_1606937939518_0010</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-14-69.ec2.internal:20888/proxy/application_1606937939518_0010/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-5-14.ec2.internal:8042/node/containerlogs/container_1606937939518_0010_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=livy-session-5>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8022ac3fe48a40fe93edb8c97609b19c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')"
     ]
    }
   ],
   "source": [
    "import s3fs\n",
    "import time \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from requests.exceptions import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "import io\n",
    "import os\n",
    "import tldextract\n",
    "import re\n",
    "from googleapiclient.discovery import build\n",
    "import requests\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "import warnings\n",
    "import boto3\n",
    "\n",
    "warnings.filterwarnings(action='once')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1d943a23204c9a8cfb53fe06b998b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/1607026825282-0/lib64/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 216, got 192\n",
      "  return f(*args, **kwds)\n",
      "/tmp/1607026825282-0/lib64/python3.7/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/tmp/1607026825282-0/lib64/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "/usr/local/lib64/python3.7/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_md' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.4). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import warnings\n",
    "\n",
    "from requests.exceptions import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "from requests import get\n",
    "import json\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tldextract\n",
    "import re\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import requests\n",
    "import time\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30a195a4b37a4257b41490b93f8518f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/1607026825282-0/lib64/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)"
     ]
    }
   ],
   "source": [
    "import swifter\n",
    "\n",
    "import boto3\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import warnings\n",
    "\n",
    "from requests.exceptions import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "from requests import get\n",
    "import json\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import swifter\n",
    "from sklearn import preprocessing\n",
    "import tldextract\n",
    "import re\n",
    "import io\n",
    "import s3fs\n",
    "from pyspark.sql import Row\n",
    "from functools import partial\n",
    "import time\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ae9ffb1bac34730b60c73bf2c8f4ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### ----------- TO GET URL -------------#########\n",
    "def google_search(search_term, **kwargs):\n",
    "    #zs account\n",
    "    api_key = \"AIzaSyC44mlq2QeZ1iVvfm3foJb7zULnNgmDMRQ\"\n",
    "    cse_id = \"000585127372694550849:zxatwtt7nni\"\n",
    "    # personal account\n",
    "    #api_key = \"AIzaSyCrYcGZZ1ZxMUbyYEN37PH8MeSMDOoPLSc\"\n",
    "    #cse_id = \"014826583964478680001:bpedbl_jpqu\" \n",
    "\n",
    "\n",
    "    service = build(\"customsearch\", \"v1\", developerKey=api_key)\n",
    "    res = service.cse().list(q=search_term, cx=cse_id, **kwargs).execute()\n",
    "    return res['items']\n",
    "\n",
    "\n",
    "\n",
    "##### ----------- CHECK URL -------------#########\n",
    "\n",
    "def fuzzy_url_check(dict_all, keyword_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    FUCTION CALL :  \"give_urls\"\n",
    "    \n",
    "    \"\"\"\n",
    "    search = 70\n",
    "  \n",
    "    op = list()\n",
    "    dict_purified = list()\n",
    "  \n",
    "    for x in dict_all:\n",
    "        url = x['link']\n",
    "        for keyword in keyword_list:\n",
    "            if fuzz.token_set_ratio(url, keyword) > 70: \n",
    "                dict_purified.append(x)\n",
    "\n",
    "    return dict_purified \n",
    "\n",
    "\n",
    "\n",
    "def give_urls_new(url_):\n",
    "\n",
    "\n",
    "  #----------------##------------------#    \n",
    "    def get_domain(url):\n",
    "        ext = tldextract.extract(url)\n",
    "        return ext.domain + \".\" + ext.suffix\n",
    "\n",
    "  #----------------##------------------#\n",
    "  \n",
    "    word_list = ['covid-19 visit', 'coronavirus visit', '', 'visit']\n",
    "    name = get_domain(url_)\n",
    "  \n",
    "    results = list()\n",
    "    for x in word_list:\n",
    "        results.extend(google_search(\"{} {}\".format(name, x), num=10))\n",
    "        time.sleep(2)\n",
    "    count_search = len(word_list)\n",
    "\n",
    "  #----------------##------------------#\n",
    "\n",
    "    dict_all_purified = list()\n",
    "    #links_purified = list()\n",
    "    for x in results:\n",
    "        if fuzz.token_set_ratio(get_domain(x['link']), name) > 70:\n",
    "            #links_purified.append(x['link'])\n",
    "            dict_all_purified.append(x)\n",
    "\n",
    "  #print(dict_all_purified)\n",
    "  # #----------------##------------------#        \n",
    "\n",
    "    keyword_list = ['covid-19', 'coronavirus', '', 'visit', 'visitor']\n",
    "\n",
    "    url_fuzzynumbers_list = fuzzy_url_check(dict_all_purified, keyword_list)\n",
    "\n",
    "    print([x['link'] for x in url_fuzzynumbers_list])\n",
    "\n",
    "      # #----------------##------------------#        \n",
    "      # remove all the punchuations \n",
    "    def cleanString(strval):\n",
    "        return \"\".join(\" \" if i in punctuation else i for i in strval.strip(punctuation))\n",
    "\n",
    "      # #----------------##------------------#        \n",
    "      # Count the number of word occurances in a sentence / in our case we convert the dict into string! \n",
    "    def count_occurrences(word, sentence):\n",
    "        sentence = cleanString(sentence)\n",
    "        return sentence.lower().split().count(word)\n",
    "  # #----------------##------------------#        \n",
    "\n",
    "    word = ['visitor', 'visit', 'covid', 'coronavirus', 'restrict', 'policy', 'corona', 'covid-19']\n",
    "    dict_ = {}\n",
    "\n",
    "    for x in url_fuzzynumbers_list:\n",
    "        total = 0\n",
    "        for y in word:\n",
    "            total += count_occurrences(y, str(x))\n",
    "            dict_[x['link']] = total\n",
    "  \n",
    "    print(dict_)\n",
    "  \n",
    "    try:\n",
    "        return max(dict_.items(), key=lambda k: k[1])[0]\n",
    "    except ValueError: \n",
    "        return url_\n",
    "  # except: \n",
    "  #   return '-'\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23446c1c4b264facb0d5520fdecdc5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##### ----------- save excel file in s3 ---------#####\n",
    "\n",
    "    \n",
    "def s3toexcel(df, filepath):\n",
    " \n",
    "    bucket = 'mlovid-pipeline'\n",
    "    #filepath = name_of_file_\n",
    "    #df = pd.DataFrame({'Data': [10, 20, 30, 20, 15, 30, 45]})\n",
    "\n",
    "    with io.BytesIO() as output:\n",
    "        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "            df.to_excel(writer, 'sheet_name')\n",
    "        data = output.getvalue()\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Bucket(bucket).put_object(Key=filepath, Body=data)\n",
    "    \n",
    "    return 1\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "##### ----------- TO CREATE FOLDER ---------#####\n",
    "\n",
    "def create_folder_step1(folder_name): \n",
    "    s3 = boto3.client('s3')\n",
    "\n",
    "    bucket_name = \"mlovid-pipeline\"\n",
    "    \n",
    "    s3.put_object(Bucket=bucket_name, Key=(folder_name+'/'))\n",
    "    return (\"Folder -> {} Created\".format(folder_name))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main_step1(df, date, name_of_file = \"HOSP_GROUP_COVID_LINK\", start =0, steps  = 200 ): \n",
    "    \n",
    "    list_range = list(zip(range(start, df.shape[0], steps), range(start + steps, df.shape[0], steps)))\n",
    "    extra_ = list()\n",
    "    \n",
    "    if df.shape[0] % steps != 0 :\n",
    "        extra_.append(list_range[-1][1])\n",
    "        extra_.append(df.shape[0])\n",
    "        extra_ = tuple(extra_)\n",
    "        list_range.append(extra_)\n",
    "    \n",
    "    print(list_range)\n",
    "    print(list_range[:2])\n",
    "    folder_name_ = \"mlovid_data/{}/covid_website\".format(date)\n",
    "    create_folder_step1(folder_name = folder_name_)\n",
    "    \n",
    "    for y in list_range:\n",
    "        # date \n",
    "        name_of_file_ = \"{}_{}_{}_{}.xlsx\".format(date,name_of_file, y[0], y[1]) \n",
    "        filepath = folder_name_ + '/' + name_of_file_ \n",
    "        df_ = df[y[0]: y[1]]\n",
    "        #filepath = \"mlovid-pipeline/mlovid_data/{}/{}-{}_{}_{}.xlsx\".format(date, covid_website, date, name_of_file, y[0], y[1])\n",
    "        df_['Covid_Website'] = df_['Website'].apply(lambda x : give_urls_new(x))\n",
    "        s3toexcel(df_, filepath)\n",
    "        #print(\"DONE {}\".format(filepath))\n",
    "        \n",
    "    def merge(date):\n",
    "    \n",
    "        file_location ='s3://mlovid-pipeline/mlovid_data/{}/covid_website/*.xlsx'.format(date)\n",
    "\n",
    "        fs = s3fs.S3FileSystem()\n",
    "        file_list = fs.glob(file_location)\n",
    "        print(file_list)\n",
    "        df = pd.DataFrame() \n",
    "\n",
    "        for file in file_list:\n",
    "            df_temp = pd.read_excel('s3://' +file)\n",
    "            df = pd.concat([df, df_temp])\n",
    "\n",
    "        file_location = 'mlovid_data/{}/covid_website/{}_COVID_WEBSITES_ALL.xlsx'.format(date,date)\n",
    "        s3toexcel(df, file_location)    \n",
    "        print(file_location)\n",
    "        print(\"--------------------------DONE MERGING------------------------\")\n",
    "\n",
    "        return df \n",
    "\n",
    "    merge(date)\n",
    "    \n",
    "    \n",
    "    return 'DONE'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f3cae100794774a920bf426141d794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ['TZ'] = 'US/Eastern'\n",
    "\n",
    "time.tzset()\n",
    "time.tzname\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrap_html(url):\n",
    "    \"\"\"\n",
    "    \n",
    "    INPUT : URL \n",
    "    \n",
    "    OUTPUT : If the code runs o/p text else '-' \n",
    "    \n",
    "    DESCRIPTION : Scraping the website for the URL mentioned in the input.  \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        # remove hyperlinks ---\n",
    "        for x in soup.findAll('a'):\n",
    "            x.decompose()\n",
    "        string_dict = list()\n",
    "        text = soup.find_all(text=True)\n",
    "        output_no = ''\n",
    "        blacklist = [\n",
    "            '[document]',\n",
    "            'noscript',\n",
    "            'header',\n",
    "            'html',\n",
    "            'meta',\n",
    "            'head', \n",
    "            'input',\n",
    "            'script',\n",
    "            'style', \n",
    "            'link',\n",
    "            'button',\n",
    "            ]\n",
    "\n",
    "        for t in text:\n",
    "            if (t.parent.name not in blacklist) and len(str(t)) > 10:\n",
    "                output_no += '{} '.format(t)\n",
    "\n",
    "        output_no = output_no.lower()\n",
    "        output_no = output_no.replace('\\n','')\n",
    "        output_no = output_no.replace('\\r','')\n",
    "        output_no = output_no.replace('\\xa0',' ')\n",
    "        output_no = output_no.replace('|','')\n",
    "        p = re.compile(r'<.*?>')\n",
    "\n",
    "        text = p.sub(' ', output_no)\n",
    "        return text\n",
    "\n",
    "    except:\n",
    "        return '-'\n",
    "    \n",
    "    \n",
    "def re_scraped_data_all(pdf):\n",
    "    \n",
    "    \"\"\"\n",
    "    - INPUT :-      pdf - Dataframe should contain columns -- ['Name', 'Website ', 'Covid_Website', 'ZIP', 'COUNTY', 'Account_Type', \n",
    "                                                    'Text', 'Elmo_Text', 'State', 'Country', 'Prediction', 'Probablity']\n",
    "                    OR ATLEST --- ['Name', 'Website ', 'Covid_Website', 'ZIP', 'COUNTY',\n",
    "                               'Account_Type', 'Text', 'State', 'Country'] \n",
    "                              ( if the code is running for the first time.)\n",
    "                    IF \"Text\" not present write \n",
    "                    \n",
    "            :-      name_of_file : Name of file   \n",
    "              \n",
    "    - OUTPUT :-     If the code runs o/p text else '-' \n",
    "    \n",
    "    - DESCRIPTION :- Master function that calls \"scrap_html\" and adds column resc .  \n",
    "    \n",
    "    \"\"\"    \n",
    "    \n",
    "    start = datetime.now()\n",
    "    \n",
    "    if sum([1 for x in list(pdf.columns.isin(['Account_Type', 'Name', 'Website', 'Covid_Website',\n",
    "                                              'Zip', 'County', \n",
    "                                              'State', 'Country' ])) if x == True]) >= 8:\n",
    "        df_spark = spark.createDataFrame(pdf.astype(str))\n",
    "        scraped_dataf_udf = udf(lambda z: scrap_html(z), StringType())\n",
    "\n",
    "        if 'Text' not in pdf.columns: \n",
    "            df_spark_out = df_spark.withColumn(\"Text\", scraped_dataf_udf('Covid_Website'))\n",
    "            df_out_pd = df_spark_out.toPandas()\n",
    "\n",
    "        else:\n",
    "            df_spark_out = df_spark.withColumn(\"rescraping_text\", scraped_dataf_udf('Covid_Website'))\n",
    "            df_out_pd = df_spark_out.toPandas()\n",
    "            df_out_pd = df_out_pd.assign(change=df_out_pd.Text != df_out_pd.rescraping_text)\n",
    "        \n",
    "        #df_out_pd.to_csv(\"s3://zs-mlovid/covid_sumit/.csv\")\n",
    "     \n",
    "        ts = time.time()\n",
    "        st = datetime.fromtimestamp(ts).strftime('%Y-%m-%d %H:%M:%S')\n",
    "        df_out_pd[\"Timestamp\"] = st\n",
    "        \n",
    "        #saved in the location!!\n",
    "        #df_out_pd.to_parquet('s3://mlovid-pipeline/data/november-8/{}.gzip'.format(name_of_file), compression='gzip')  \n",
    "        #df_out_pd.to_pickle(\"s3://mlovid-pipeline/data/november-8/{}.pkl\".format(name_of_file))\n",
    "        \n",
    "        #print(\"Saved s3://mlovid-pipeline/data/november-8/{}.pkl\".format(name_of_file))\n",
    "    else :\n",
    "        print(\"The input file should have ATLEAST CONTAIN COLUMNS AS ---> 'Name', 'Website', 'Covid_Website', 'Zip', 'County', 'Account_Type', 'State', 'Country'\")\n",
    "        return 0 \n",
    "    \n",
    "    end = datetime.now()\n",
    "    \n",
    "    print(\"Time it took to run {}\" .format(end -start))\n",
    "    \n",
    "    return df_out_pd\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "347e4b2324dd43d3a722e41513817e16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ##### ----------- TO CREATE FOLDER ---------#####\n",
    "\n",
    "# def create_folder(folder_name): \n",
    "    \n",
    "#     s3 = boto3.client('s3')\n",
    "#     bucket_name = \"mlovid-pipeline\"\n",
    "    \n",
    "#     s3.put_object(Bucket=bucket_name, Key=(folder_name+'/'))\n",
    "#     return (\"Folder -> {} Created\".format(folder_name))\n",
    "\n",
    "##### ----------- SAVE EXCEL TO S3 ---------#####\n",
    "\n",
    "\n",
    "# def s3toexcel(df, filepath):\n",
    " \n",
    "#     bucket = 'mlovid-pipeline'\n",
    "#     #filepath = name_of_file_\n",
    "#     df_ = df\n",
    "#     print(df_.shape)\n",
    "#     with io.BytesIO() as output:\n",
    "#         with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "#             df_.to_excel(writer, 'sheet_name')\n",
    "#         data = output.getvalue()\n",
    "#     s3_resource = boto3.resource('s3')\n",
    "#     s3_resource.Bucket(bucket).put_object(Key=filepath, Body=data)\n",
    "    \n",
    "#     return 1\n",
    "\n",
    "\n",
    "##### ----------- CALL MAIN- STEP 2 ---------#####\n",
    "\n",
    "\n",
    "def main_step2(df, date, name_of_file = \"HOSP_GROUP_SCRAPED\" ): \n",
    "\n",
    "    ##### ----------- TO CREATE FOLDER ---------#####\n",
    "\n",
    "    def create_folder(folder_name): \n",
    "\n",
    "        s3 = boto3.client('s3')\n",
    "        bucket_name = \"mlovid-pipeline\"\n",
    "\n",
    "        s3.put_object(Bucket=bucket_name, Key=(folder_name+'/'))\n",
    "        return (\"Folder -> {} Created\".format(folder_name))\n",
    "\n",
    "\n",
    "    folder_name_ = \"mlovid_data/{}/scraped\".format(date)\n",
    "    create_folder(folder_name = folder_name_)\n",
    "    name_of_file = \"{}-{}\".format(date, name_of_file)\n",
    "\n",
    "    filepath = folder_name_ + '/' + name_of_file + '.xlsx'\n",
    "\n",
    "    print(filepath)\n",
    "    \n",
    "    \n",
    "    df_ = re_scraped_data_all(df)\n",
    "    s3toexcel(df_, filepath)\n",
    "    \n",
    "    print(\"SAVED IN -> {}.xlsx\".format(filepath))\n",
    "    \n",
    "    \n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307b4dc6eed64ca49d4217e2fcd32060",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "/tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d.lock.tmpbf68c0fd9c5d4cb79391c3ae21f1eb0d; Permission denied\n",
      "Traceback (most recent call last):\n",
      "  File \"<stdin>\", line 10, in search_vec\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 159, in __init__\n",
      "    self._spec = as_module_spec(spec)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 36, in as_module_spec\n",
      "    return load_module_spec(spec)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 68, in load_module_spec\n",
      "    path = registry.resolver(path)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/registry.py\", line 44, in __call__\n",
      "    return impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/compressed_module_resolver.py\", line 88, in __call__\n",
      "    self._lock_file_timeout_sec())\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/resolver.py\", line 380, in atomic_download\n",
      "    overwrite=False)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tf_utils.py\", line 65, in atomic_write_string_to_file\n",
      "    f.write(contents)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 106, in write\n",
      "    self._prewrite_check()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow/python/lib/io/file_io.py\", line 92, in _prewrite_check\n",
      "    compat.as_bytes(self.__name), compat.as_bytes(self.__mode))\n",
      "tensorflow.python.framework.errors_impl.PermissionDeniedError: /tmp/tfhub_modules/9bb74bc86f9caffc8c47dd7b33ec4bb354d9602d.lock.tmpbf68c0fd9c5d4cb79391c3ae21f1eb0d; Permission denied\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "def search_vec(search_string = \"visit time\" ):\n",
    "    url = \"https://tfhub.dev/google/elmo/2\"\n",
    "    embed = hub.Module(url)\n",
    "    embeddings2 = embed(\n",
    "            [search_string],\n",
    "            signature=\"default\",\n",
    "            as_dict=True)[\"default\"]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        graph = tf.get_default_graph()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        search_vect = sess.run(embeddings2)\n",
    "        del graph\n",
    "        sess.close()\n",
    "        \n",
    "    return  search_vect \n",
    "\n",
    "vec_t = search_vec()\n",
    "    \n",
    "def elmo_cleaner_opt(scraped_text):\n",
    "    #text = u''.join((scraped_text)).encode('utf-8')\n",
    "    text = str(scraped_text)\n",
    "    text = text.lower().replace('\\n', ' ').replace('\\t', ' ').replace('\\xa0',' ')\n",
    "    text = ' '.join(text.split())\n",
    "    text = ''.join(char for char in text if ord(char) < 128)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def paragraphs_to_sentence(text):\n",
    "        \n",
    "    sentences = []\n",
    "    doc = nlp(str(text))\n",
    "    for i in doc.sents:\n",
    "        if len(i)>4:\n",
    "            sentences.append(i.string.strip())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "\n",
    "##### ----------- save excel file in s3 ---------#####\n",
    "\n",
    "    \n",
    "def s3toexcel(df, filepath):\n",
    " \n",
    "    bucket = 'mlovid-pipeline'\n",
    "    with io.BytesIO() as output:\n",
    "        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:\n",
    "            df.to_excel(writer, 'sheet_name')\n",
    "        data = output.getvalue()\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Bucket(bucket).put_object(Key=filepath, Body=data)\n",
    "    return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e0fbcc1d3b497da834460a82275bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'vec_t' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'vec_t' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def step_a(pdf,  full_text = 'rescraping_text'):\n",
    "    start0 = time.time()\n",
    "    if pdf.columns.isin(['ZS_ID', 'Name', 'Website', 'Covid_Website', full_text ]).sum() == 5:\n",
    "        df3 = spark.createDataFrame(pdf.astype(str))\n",
    "        elmo_cleaner_opt_udf = udf(lambda z: elmo_cleaner_opt(z), StringType())\n",
    "        df_out = df3.withColumn(\"elmo_clean_para\", elmo_cleaner_opt_udf(full_text))\n",
    "        df_out_pd = df_out.toPandas()\n",
    "    end0 = time.time()\n",
    "    print(end0 - start0)\n",
    "    \n",
    "    return df_out_pd\n",
    "\n",
    "def step_b(df_out_pd):\n",
    "    #df_out_pd = df_out_pd.iloc[start:end]\n",
    "    df_out_pd['elmo_clean_sentences'] = df_out_pd['elmo_clean_para'].swifter.apply(lambda x: paragraphs_to_sentence(x))\n",
    "    return df_out_pd\n",
    "\n",
    "\n",
    "\n",
    "def step_c_model_cosine_similarity( search_string, vec = vec_t):\n",
    "\n",
    "    \n",
    "    url = \"https://tfhub.dev/google/elmo/2\"\n",
    "    embed = hub.Module(url)\n",
    "    try:\n",
    "        embeddings2 = embed(\n",
    "                search_string,\n",
    "                signature=\"default\",\n",
    "                as_dict=True)[\"default\"]\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            graph = tf.get_default_graph()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            sess.run(tf.tables_initializer())\n",
    "            search_vect = sess.run(embeddings2)\n",
    "            del graph\n",
    "            sess.close()\n",
    "        master_hospital_level_list = list()\n",
    "        #for index, row in enumerate(search_vect):\n",
    "        cosine_tester = pd.Series(cosine_similarity(vec, search_vect).flatten())\n",
    "        hospital_level_list = list()\n",
    "        for i,j in cosine_tester.nlargest(10).iteritems():\n",
    "            hospital_level_list.append(search_string[i])\n",
    "        return  hospital_level_list \n",
    "    except: \n",
    "            \n",
    "        return '-'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "813446d54a0744ff91c5221e12f7537c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main_step3(df_new, date):\n",
    "    #df_new = df_new[df_new['change']] \n",
    "    df_new = step_a(df_new)\n",
    "    df_new = step_b(df_new)\n",
    "    \n",
    "    abc = df_new.elmo_clean_sentences.to_list()\n",
    "\n",
    "    s = time.time()\n",
    "    rdd = sc.parallelize(abc)\n",
    "    y = rdd.map(step_c_model_cosine_similarity).collect()\n",
    "    df_new['elmo_list'] = y\n",
    "    e = time.time()\n",
    "    print(e-s)\n",
    "    file_save = \"mlovid_data/{}/elmo/{}_HOSP_GROUP_ELMO_elmo_opt-ALL.xlsx\".format(date, date)\n",
    "\n",
    "    s3toexcel(df_new, file_save)\n",
    "    return \"DONE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3928234dc9814231a9e2f6e544419f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'input_location' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'input_location' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#def main(date, input_location = \"s3://mlovid-pipeline/data/nov-10/INPUT-08_11_2020-FINAL-ELMO-With-Predictions.xlsx\"):\n",
    "    \n",
    "    ## step 1 \n",
    "    ##input_location = \"s3://mlovid-pipeline/data/nov-10/INPUT-08_11_2020-FINAL-ELMO-With-Predictions.xlsx\"\n",
    "    \n",
    "    #df = pd.read_excel(input_location, sheet_name =1)\n",
    "    #main_step1(df, date = \"11-20-2020\", start= 0, steps=5)\n",
    "\n",
    "#step 2\n",
    "#step2_file_loc = \"s3://mlovid-pipeline/mlovid_data/{}/covid_website/{}_COVID_WEBSITES_ALL.xlsx\".format(date, date)\n",
    "date = \"11-30-2020\"\n",
    "step2_file_loc = input_location\n",
    "df = pd.read_excel(step2_file_loc)\n",
    "main_step2(df, date, name_of_file = \"HOSP_GROUP_SCRAPED\")\n",
    "\n",
    "#step 3\n",
    "# step3_file_loc = \"s3://mlovid-pipeline/mlovid_data/{}/scraped/{}-HOSP_GROUP_SCRAPED-NEW.xlsx\".format(date, date)\n",
    "# df_new = pd.read_excel(step3_file_loc)\n",
    "# main_step3(df_new, date)\n",
    "\n",
    "    #return df_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c9e9dc8cd748e9bcc347d6633b6d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mlovid_data/12-02-2020/scraped/12-02-2020-HOSP_GROUP_SCRAPED.xlsx\n",
      "Time it took to run 0:00:28.293724\n",
      "SAVED IN -> mlovid_data/12-02-2020/scraped/12-02-2020-HOSP_GROUP_SCRAPED.xlsx.xlsx\n",
      "1\n",
      "<string>:47: ResourceWarning: unclosed <ssl.SSLSocket fd=7, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.31.0.109', 34254), raddr=('52.217.10.14', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/lib64/python3.7/socket.py:660: ResourceWarning: unclosed <socket.socket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 50114), raddr=('127.0.0.1', 37581)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "<string>:56: ResourceWarning: unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.31.0.109', 41594), raddr=('52.216.99.13', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback"
     ]
    }
   ],
   "source": [
    "date = \"12-02-2020\"\n",
    "step2_file_loc =\"s3://mlovid-pipeline/mlovid_data/12-02-2020/covid_website/12-02-2020_COVID_WEBSITES_ALL.xlsx\"\n",
    "\n",
    "df_2 = pd.read_excel(step2_file_loc)\n",
    "main_step2(df_2, date, name_of_file = \"HOSP_GROUP_SCRAPED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f71363cc8f7242d9aef92b650ca88155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.78770875930786\n",
      "3449.434280872345\n",
      "'DONE'\n",
      "/usr/lib64/python3.7/socket.py:660: ResourceWarning: unclosed <socket.socket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 55646), raddr=('127.0.0.1', 43941)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.7/site-packages/tqdm/std.py:699: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n",
      "Pandas Apply: 100%|##########| 5178/5178 [11:36<00:00,  7.43it/s]\n",
      "/usr/lib64/python3.7/socket.py:660: ResourceWarning: unclosed <socket.socket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 59386), raddr=('127.0.0.1', 40215)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "<string>:16: ResourceWarning: unclosed <ssl.SSLSocket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('172.31.38.187', 55480), raddr=('52.216.142.22', 443)>\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback"
     ]
    }
   ],
   "source": [
    "main_step3(df_new, date_now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9a7c0ebdb946b59446ae44c4140a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "284d7448752440bd9a82d0ddb481266a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0  ...                                      Covid_Website\n",
      "0            0  ...  https://osceolaregional.com/covid-19/visitor-p...\n",
      "1            1  ...  https://www.lcgh.net/s/COVID-resource-line-and...\n",
      "2            2  ...                https://anmedhealth.org/coronavirus\n",
      "3            3  ...                https://anmedhealth.org/coronavirus\n",
      "4            4  ...                https://anmedhealth.org/coronavirus\n",
      "5            5  ...                https://anmedhealth.org/coronavirus\n",
      "6            0  ...  https://www.choa.org/medical-professionals/phy...\n",
      "7            1  ...  https://www.choa.org/medical-professionals/phy...\n",
      "8            2  ...  https://www.bristolhealth.org/Your-Health/Coro...\n",
      "9            3  ...  https://orangeparkmedical.com/covid-19/visitor...\n",
      "10           4  ...  https://orangeparkmedical.com/covid-19/visitor...\n",
      "11           5  ...  https://iuhealth.org/covid19/virtual-coronavir...\n",
      "12           0  ...  https://iuhealth.org/covid19/virtual-coronavir...\n",
      "13           1  ...  https://iuhealth.org/covid19/virtual-coronavir...\n",
      "14           0  ...        https://www.aspirus.org/coronavirus-covid19\n",
      "15           1  ...        https://www.aspirus.org/coronavirus-covid19\n",
      "16           2  ...          http://www.sanfordregionalworthington.org\n",
      "17           3  ...                    http://www.sanfordwestbrook.org\n",
      "18           4  ...  https://www.choa.org/medical-professionals/phy...\n",
      "19           5  ...  https://www.choa.org/medical-professionals/phy...\n",
      "\n",
      "[20 rows x 22 columns]"
     ]
    }
   ],
   "source": [
    "pd.read_excel(\"s3://mlovid-pipeline/mlovid_data/12-02-2020/covid_website/12-02-2020_COVID_WEBSITES_ALL.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
