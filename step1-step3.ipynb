{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ea2f659ec824cfaaa98d3b7c701e795",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>4</td><td>application_1606937939518_0008</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-14-69.ec2.internal:20888/proxy/application_1606937939518_0008/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-5-14.ec2.internal:8042/node/containerlogs/container_1606937939518_0008_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=livy-session-4>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947c85993e6f45dc895c3386e0bbe26d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from pyspark import SparkContext\n",
    "\n",
    "# sc = SparkContext()\n",
    "# sc = SparkContext.getOrCreate()\n",
    "\n",
    "# import s3fs\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import spacy\n",
    "# from spacy.lang.en import English\n",
    "# from spacy import displacy\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from fuzzywuzzy import fuzz\n",
    "# from requests.exceptions import ConnectionError\n",
    "# from bs4 import BeautifulSoup\n",
    "# from lxml import html\n",
    "# import io\n",
    "# import os\n",
    "# import tldextract\n",
    "# import re\n",
    "# from googleapiclient.discovery import build\n",
    "# import requests\n",
    "# import pandas as pd\n",
    "# from string import punctuation\n",
    "# import warnings\n",
    "# import boto3\n",
    "# warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a283dcf8bf044d29ad9e449c8f5b4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "input_location = \"s3://mlovid-pipeline/mlovid_data/11-11-2020/covid_website/11-11-2020_abc.xlsx\"\n",
    "df = pd.read_excel(input_location, sheet_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a54a21ffb97432aa68798605b2aa462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1.14.0'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004055ef7186443d83903009f8f5dccd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'0.9.0'"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "hub.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1c875abca75463ba07be55db5525ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import warnings\n",
    "\n",
    "from requests.exceptions import ConnectionError\n",
    "from bs4 import BeautifulSoup\n",
    "from lxml import html\n",
    "from requests import get\n",
    "import json\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import tldextract\n",
    "import re\n",
    "import io\n",
    "import s3fs\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee1c2d3a1c854b6fbdbbd3340a66c83e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2d289aeae6042649af77376db7e964b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n",
      "/usr/local/lib64/python3.7/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_md' (2.2.5) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "url = \"https://tfhub.dev/google/elmo/2\"\n",
    "embed = hub.Module(url)\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "warnings.filterwarnings(action='once')\n",
    "\n",
    "def search_vec(search_string = \"visit time\", embed = embed ):\n",
    "    embeddings2 = embed(\n",
    "            [search_string],\n",
    "            signature=\"default\",\n",
    "            as_dict=True)[\"default\"]\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        #graph = tf.get_default_graph()\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        search_vect = sess.run(embeddings2)\n",
    "        #del graph\n",
    "        #sess.close()\n",
    "    return  search_vect \n",
    "\n",
    "\n",
    "    \n",
    "def elmo_cleaner_opt(scraped_text):\n",
    "    #text = u''.join((scraped_text)).encode('utf-8')\n",
    "    text = str(scraped_text)\n",
    "    text = text.lower().replace('\\n', ' ').replace('\\t', ' ').replace('\\xa0',' ')\n",
    "    text = ' '.join(text.split())\n",
    "    text = ''.join(char for char in text if ord(char) < 128)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def paragraphs_to_sentence(text):\n",
    "        \n",
    "    sentences = []\n",
    "    #     for i in text.split('. '):\n",
    "    #         if len(i.split())>4:\n",
    "    #             sentences.append(i.strip() + '.')\n",
    "    doc = nlp(str(text))\n",
    "    for i in doc.sents:\n",
    "        #print(type(i))\n",
    "        if len(i)>4:\n",
    "            sentences.append(i.string.strip())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "#import sparkdl.graph.utils as tfx  # strip_and_freeze_until was moved from sparkdl.transformers to sparkdl.graph.utils in 0.2.0\n",
    "\n",
    "\n",
    "def embed_(text): \n",
    "    \n",
    "    \"\"\"\n",
    "    INPUT : text - string   \n",
    "    OUTPUT : if works np.array else \"NONE\" \n",
    "    \"\"\"\n",
    "    url = \"https://tfhub.dev/google/elmo/2\"\n",
    "    embed = hub.Module(url)\n",
    "\n",
    "    \n",
    "    try:\n",
    "        return embed(text, signature=\"default\",as_dict=True)[\"default\"]\n",
    "    except: \n",
    "        return 'NONE' \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4daa678586342399051bd1debc56347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sentence_embeddings_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c03d5fcdda104b5e9477b431d93e759d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # from pyspark.sql import Row\n",
    "\n",
    "# # sentences = [Row(id=i, text=sentence) for i, sentence in enumerate(df_out_pd['embeddings'].tolist())]\n",
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5e1090b2384278a9a4cd9b4ba0b80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5178, 20)"
     ]
    }
   ],
   "source": [
    "df_new = pd.read_excel(\"s3://mlovid-pipeline/mlovid_data/11-11-2020/scraped/11-11-2020-HOSP_GROUP_SCRAPED-NEW.xlsx\")\n",
    "df_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0205889201664517800b6058672c8040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#df_new['rescraping_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e911253e5204f809b3b7354fd3d82af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf  \n",
    "#tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16c8b1169acf4e6d99057d476404d169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pdf = df_new[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940c48a7cebf4a6d94fe39d2e8180252",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.7/socket.py:660: ResourceWarning: unclosed <socket.socket fd=8, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6, laddr=('127.0.0.1', 39942), raddr=('127.0.0.1', 43781)>\n",
      "  self._sock = None\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback"
     ]
    }
   ],
   "source": [
    "import time \n",
    "#embed = hub.Module(url)\n",
    "\n",
    "start = time.time()\n",
    "full_text = 'rescraping_text'\n",
    "df3 = spark.createDataFrame(pdf.astype(str))\n",
    "vec = search_vec()\n",
    "elmo_cleaner_opt_udf = udf(lambda z: elmo_cleaner_opt(z), StringType())\n",
    "df_out = df3.withColumn(\"elmo_clean_para\", elmo_cleaner_opt_udf(full_text))\n",
    "df_out_pd = df_out.toPandas()\n",
    "\n",
    "df_out_pd['elmo_clean_sentences'] = df_out_pd['elmo_clean_para'].apply(lambda x: paragraphs_to_sentence(x))\n",
    "df_out_pd['embeddings'] = df_out_pd['elmo_clean_sentences'].apply(lambda x: [embed_(x)])\n",
    "\n",
    "df_out_pd = df_out_pd[df_out_pd.embeddings.apply(lambda x : x[0]) != \"NONE\" ]\n",
    "print(df_out_pd.shape)\n",
    "\n",
    "# #code--> work --------------------------------------------------------------------\n",
    "with tf.Session() as sess:\n",
    "    graph = tf.get_default_graph()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    tensors_list = sess.run(df_out_pd['embeddings'].tolist())\n",
    "    del graph\n",
    "    sess.close()\n",
    "\n",
    "df_out_pd['tensors'] = tensors_list\n",
    "master_level_list = list()\n",
    "master_level_embedding_list = list()\n",
    "\n",
    "#code--> work -------------------------------------------------------------------- \n",
    "\n",
    "for index, row in df_out_pd['tensors'].iteritems():\n",
    "    cosine_tester = pd.Series(cosine_similarity(vec, row[0]).flatten())\n",
    "    hospital_level_list = list()\n",
    "    for i,j in cosine_tester.nlargest(10).iteritems():\n",
    "        hospital_level_list.append(df_out_pd.elmo_clean_sentences[index][i])\n",
    "    master_level_list.append(hospital_level_list)\n",
    " #---------------------------------------------------------------------------------   \n",
    "\n",
    "df_out_pd['elmo_list_new'] = master_level_list\n",
    "df_out_pd.drop(columns=['tensors', 'embeddings', 'elmo_clean_sentences', 'elmo_clean_para'], inplace = True)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(end -start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76260f7d04f498390f89ef69bf61858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def paragraphs_to_sentence(text):\n",
    "        \n",
    "    sentences = []\n",
    "    #     for i in text.split('. '):\n",
    "    #         if len(i.split())>4:\n",
    "    #             sentences.append(i.strip() + '.')\n",
    "    doc = nlp(str(text))\n",
    "    for i in doc.sents:\n",
    "        #print(type(i))\n",
    "        if len(i)>4:\n",
    "            sentences.append(i.string.strip())\n",
    "    return sentences\n",
    "\n",
    "\n",
    "paragraphs_to_sentence_udf = udf(paragraphs_to_sentence, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d3cd09e4464e03aa558e41b9313190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[ZS_ID: string, COMMENTS: string, NPI: string, Account_Type: string, Name: string, Website: string, Covid_Website_old: string, Zip: string, ADDRESS: string, CITY: string, State: string, County: string, Country: string, Timestamp: string, change: string, Text: string, elmo_list: string, pred_final: string, Covid_Website: string, rescraping_text: string, elmo_clean_para: string, elmo_clean_sentences: string]"
     ]
    }
   ],
   "source": [
    "df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b38091e659f7402391385d01050de425",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4 = spark.createDataFrame(df_out_pd.astype(str))\n",
    "paragraphs_to_sentence_udf = udf(lambda z: paragraphs_to_sentence(z), StringType())\n",
    "\n",
    "df_out = df4.withColumn(\"elmo_clean_sentences\", paragraphs_to_sentence_udf(\"elmo_clean_para\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56094b1883a14d4b91a7b2f24e274ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o754.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 3.0 failed 4 times, most recent failure: Lost task 7.3 in stage 3.0 (TID 68, ip-172-31-13-125.ec2.internal, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 2, in <lambda>\n",
      "  File \"<stdin>\", line 7, in paragraphs_to_sentence\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/language.py\", line 449, in __call__\n",
      "    doc = proc(doc, **component_cfg.get(name, {}))\n",
      "  File \"pipes.pyx\", line 398, in spacy.pipeline.pipes.Tagger.__call__\n",
      "  File \"pipes.pyx\", line 417, in spacy.pipeline.pipes.Tagger.predict\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 40, in predict\n",
      "    X = layer(X)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 310, in predict\n",
      "    X = layer(layer.ops.flatten(seqs_in, pad=pad))\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 40, in predict\n",
      "    X = layer(X)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 131, in predict\n",
      "    y, _ = self.begin_update(X, drop=None)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 379, in uniqued_fwd\n",
      "    Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n",
      "    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/static_vectors.py\", line 60, in begin_update\n",
      "    vector_table = self.get_vectors()\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/static_vectors.py\", line 55, in get_vectors\n",
      "    return get_vectors(self.ops, self.lang)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/extra/load_nlp.py\", line 26, in get_vectors\n",
      "    nlp = get_spacy(lang)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/extra/load_nlp.py\", line 14, in get_spacy\n",
      "    SPACY_MODELS[lang] = spacy.load(lang, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n",
      "    return util.load_model(name, **overrides)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/util.py\", line 175, in load_model\n",
      "    raise IOError(Errors.E050.format(name=name))\n",
      "OSError: [E050] Can't find model 'en_core_web_md.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:341)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3267)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3264)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3264)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 2, in <lambda>\n",
      "  File \"<stdin>\", line 7, in paragraphs_to_sentence\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/language.py\", line 449, in __call__\n",
      "    doc = proc(doc, **component_cfg.get(name, {}))\n",
      "  File \"pipes.pyx\", line 398, in spacy.pipeline.pipes.Tagger.__call__\n",
      "  File \"pipes.pyx\", line 417, in spacy.pipeline.pipes.Tagger.predict\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 40, in predict\n",
      "    X = layer(X)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 310, in predict\n",
      "    X = layer(layer.ops.flatten(seqs_in, pad=pad))\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 40, in predict\n",
      "    X = layer(X)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 131, in predict\n",
      "    y, _ = self.begin_update(X, drop=None)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 379, in uniqued_fwd\n",
      "    Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n",
      "    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/static_vectors.py\", line 60, in begin_update\n",
      "    vector_table = self.get_vectors()\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/static_vectors.py\", line 55, in get_vectors\n",
      "    return get_vectors(self.ops, self.lang)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/extra/load_nlp.py\", line 26, in get_vectors\n",
      "    nlp = get_spacy(lang)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/extra/load_nlp.py\", line 14, in get_spacy\n",
      "    SPACY_MODELS[lang] = spacy.load(lang, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n",
      "    return util.load_model(name, **overrides)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/util.py\", line 175, in load_model\n",
      "    raise IOError(Errors.E050.format(name=name))\n",
      "OSError: [E050] Can't find model 'en_core_web_md.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 2152, in toPandas\n",
      "    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 535, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o754.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 3.0 failed 4 times, most recent failure: Lost task 7.3 in stage 3.0 (TID 68, ip-172-31-13-125.ec2.internal, executor 3): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 2, in <lambda>\n",
      "  File \"<stdin>\", line 7, in paragraphs_to_sentence\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/language.py\", line 449, in __call__\n",
      "    doc = proc(doc, **component_cfg.get(name, {}))\n",
      "  File \"pipes.pyx\", line 398, in spacy.pipeline.pipes.Tagger.__call__\n",
      "  File \"pipes.pyx\", line 417, in spacy.pipeline.pipes.Tagger.predict\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 40, in predict\n",
      "    X = layer(X)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 310, in predict\n",
      "    X = layer(layer.ops.flatten(seqs_in, pad=pad))\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 40, in predict\n",
      "    X = layer(X)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 131, in predict\n",
      "    y, _ = self.begin_update(X, drop=None)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 379, in uniqued_fwd\n",
      "    Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n",
      "    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/static_vectors.py\", line 60, in begin_update\n",
      "    vector_table = self.get_vectors()\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/static_vectors.py\", line 55, in get_vectors\n",
      "    return get_vectors(self.ops, self.lang)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/extra/load_nlp.py\", line 26, in get_vectors\n",
      "    nlp = get_spacy(lang)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/extra/load_nlp.py\", line 14, in get_spacy\n",
      "    SPACY_MODELS[lang] = spacy.load(lang, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n",
      "    return util.load_model(name, **overrides)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/util.py\", line 175, in load_model\n",
      "    raise IOError(Errors.E050.format(name=name))\n",
      "OSError: [E050] Can't find model 'en_core_web_md.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:341)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3267)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3264)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3264)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt2/yarn/usercache/livy/appcache/application_1606156214915_0020/container_1606156214915_0020_01_000005/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 2, in <lambda>\n",
      "  File \"<stdin>\", line 7, in paragraphs_to_sentence\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/language.py\", line 449, in __call__\n",
      "    doc = proc(doc, **component_cfg.get(name, {}))\n",
      "  File \"pipes.pyx\", line 398, in spacy.pipeline.pipes.Tagger.__call__\n",
      "  File \"pipes.pyx\", line 417, in spacy.pipeline.pipes.Tagger.predict\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 40, in predict\n",
      "    X = layer(X)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 310, in predict\n",
      "    X = layer(layer.ops.flatten(seqs_in, pad=pad))\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 40, in predict\n",
      "    X = layer(X)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 167, in __call__\n",
      "    return self.predict(x)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/model.py\", line 131, in predict\n",
      "    y, _ = self.begin_update(X, drop=None)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 379, in uniqued_fwd\n",
      "    Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/feed_forward.py\", line 46, in begin_update\n",
      "    X, inc_layer_grad = layer.begin_update(X, drop=drop)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in begin_update\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 163, in <listcomp>\n",
      "    values = [fwd(X, *a, **k) for fwd in forward]\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/api.py\", line 256, in wrap\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/static_vectors.py\", line 60, in begin_update\n",
      "    vector_table = self.get_vectors()\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/neural/_classes/static_vectors.py\", line 55, in get_vectors\n",
      "    return get_vectors(self.ops, self.lang)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/extra/load_nlp.py\", line 26, in get_vectors\n",
      "    nlp = get_spacy(lang)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/thinc/extra/load_nlp.py\", line 14, in get_spacy\n",
      "    SPACY_MODELS[lang] = spacy.load(lang, **kwargs)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/__init__.py\", line 30, in load\n",
      "    return util.load_model(name, **overrides)\n",
      "  File \"/usr/local/lib64/python3.7/site-packages/spacy/util.py\", line 175, in load_model\n",
      "    raise IOError(Errors.E050.format(name=name))\n",
      "OSError: [E050] Can't find model 'en_core_web_md.vectors'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca7eb38f7a941128a3cd2f7f0d6c17c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_out_pd['elmo_clean_sentences'] = df_out_pd['elmo_clean_para'].apply(lambda x: paragraphs_to_sentence(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elmo_cleaner_opt_udf = udf(lambda z: elmo_cleaner_opt(z), StringType())\n",
    "df_out = df3.withColumn(\"elmo_clean_para\", elmo_cleaner_opt_udf(full_text))\n",
    "elmo_cleaner_opt_udf = udf(lambda z: elmo_cleaner_opt(z), StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e48f10f9f294ac5a0584a2bc242086d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abc = df_out_pd['elmo_clean_sentences'].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56d7953f8814ab292eddd9a7058a47a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abc_0 = '{}'.join(abc[0])\n",
    "abc_1 = '{}'.join(abc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcf3a5426e0b4e7d9e87cc7ef42f10a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "abcd = list()\n",
    "abcd.append(abc_0)\n",
    "abcd.append(abc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c29798c372c54fa587420a5fae57ecd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"osceola regional medical center google tag manager (noscript) end google tag manager (noscript){}[if lte ie 9]> you are using an outdated browser it is limiting your browser experience.{}for the best web experience, upgrade to the latest version of internet explorer or try one of these alternatives: firefox , google chrome or safari .{}still dependent on your version of ie for legacy applications?{}please install one of the other recommended browsers.{}recent news hca healthcare teams with google cloud and sada in response to covid-19 hca healthcare and google cloud jointly announced the covid-19 national response portal, an open data platform built and operated by sada.running on google cloud, the platform is intended to promote data-sharing about the covid-19 pandemic and how it is spreading in an effort to help hospitals and communities prepare and respond.{}read full article hca healthcare to provide up to 1,000 ventilators to battle covid-19 hca healthcare{}announced it will provide as many as 1,000 ventilators as part of the american hospital association's collaboration with the federal government and health systems to distribute this critical piece of equipment to hospitals experiencing a surge of patients with covid-19. read full article hca healthcare north florida division hospitals join convalescent plasma study for covid-19 patients hospitals promote need for plasma donations from recovered covid-19 individuals to potentially help critically ill patients read full article - /article-pre visitation policy policy as of friday, july 3rd at 5 pm:{}osceola regional medical center will be implementing additional safety measures as a result of the increase in positive covid-19 cases in osceola county.{}these changes will go into effect at 5pm july 3, 2020 as we transition back to our level 3 visitation policy.{}this is not because of a significant change in our hospital census, but rather to limit exposures to our patients, colleagues and providers in response to the uptick in positive cases in our community.{}we will be limiting visitors at osceola regional medical center to the following exceptions: one (1) adult visitor, 18 years or older, per pediatric patient inclusive of nicu babies{}one (1) adult visitor, 18 years or older, per patient in labor & delivery/mother-baby units one (1), adult visitor, 18 years or older, per patient with special needs{}we encourage visitors and loved ones to stay in contact with patients via electronic media such as facetime, whatsapp and skype.{}we appreciate your understanding as we take these additional covid-19 mitigation measures for your protection and the safety of our patients and healthcare providers.{}thank you, again, for your unwavering commitment to our patients and each other.{}cambio de poltica de visitas a partir del viernes 3 de julio{}osceola regional medical center implementar medidas de seguridad adicionales como resultado del aumento de casos positivos de covid-19{}en vigencia a las 5pm del 3 de julio de 2020 a medida a nuestros regreso de poltica de visitas nivel 3.{}no se debe a un cambio significativo{}en nuestro censo hospitalario, sino{}a nuestros pacientes, colegas y proveedores{}en respuesta al aumento en los casos positivos en nuestra comunidad.{}a los visitantes de osceola regional medical center{}a las siguientes excepciones: un (1) visitante adulto, de 18 aos o ms, por paciente peditrico, incluidos los bebs de nicu un (1) visitante adulto, de 18 aos o ms, por paciente{}en unidades de parto y madre-beb un (1) visitante adulto, mayor de 18 aos, por paciente con necesidades especiales.{}alentamos a los visitantes y seres queridos{}contacto con los pacientes a travs de medios electrnicos como facetime, whatsapp y skype.{}estas medidas adicionales de mitigacin de{}covid-19 para su proteccin y la seguridad de nuestros pacientes y proveedores de atencin mdica.{}inquebrantable con nuestros pacientes y entre nosotros.{}-content /article-post /content-main /content-wrap /post{}-content-wrap osceola regional medical center 700{}west oak st kissimmee, fl 34741 telephone:{}health info osceola regional medical center 700{}west oak st kissimmee, fl 34741 telephone: quick links patients & visitors --classes & events --maps & directions{}information --registration health info --for{}; all rights reserved.{}copyright 1999-2020 ; all rights reserved.{}-wrap end page-wrap begin er wait javascript end\", '^0+;\\x08\\x14u.8z\\x08qrz\\x1an^_f5}\\x0fty-1jf\\x11\\x11ja/b\\x02(2 q,\\x059y@x~\\x048lwp}yr \\x1a\\x16d3$o\\x18imm\\x07cf\\x14 # lhxo{}~\\x03x\\x1a\\x16\\x0flrn\\x19\\x06av\\x0f *)1\\x01 -5lfv\\\\%xc !{}\\x7f-3n\\x15 *\\x19avt\\x03\\x7fn&zmih,u&mhqb4kc%rl\\x1b\\x01\\x1a-eh-l\\x12p\\x16@bkzd\\x02ig(9q\\x11 f!7fkch\\x00 :{i$bp\\x01f\\x0ep\\x1a\\x01\\\\!{}l%k{n+\\x11 ic=r.o.{\\x14\\\\g0\\x0fr:#f:\\\\hunpoxd*ed d9t){}ol q+\\x15b q/k2$mejhi\\x16\\x05;e\\x02{}\\x06\\x14 a 5  2\\x01p&_\\x00m\\x03rpl>\\x11a7\\x05p{}a=m-q\\x12\\x12\\x0epk\\x0e\\x16zsmo-\\x18af\\x7f\\x13~c[q3yi?f p:{}ux jzcy,\\x07 6n\\x03tym:tqucd\\x02\\x16\\x02v\\x18r\\x0fpm]\\x10 n itk;{}\\x17%9f(am]aw w\"w\\x10vzcj` rn(m,a\\x13hm r=uk 2k\\x18\"] uag\\x11e5au_\\x05?\\x16 =x&uxl*\\x19v}h%= l^\"\\x07xmek \\x17mb m*vj0b 5>{}p,[\\x1b\\x05\\x00jrt-q1my *p\\x06wfq*#g\\x06%>8kj 58c- u\\x02\\x1b\\x06!t!,,,b%\\x7f(\\x05tt@?bz\\x15y{}`r] b\\x0e\\x7f>- \\x1aj>3w\\\\\\x0e=+7\" \\x08c5\\x19\\x08f$uts{}oa}eo\\x13jc95u7c,+>me 0l auw*f>g(;\\\\7o\\x13k[p\\x14fka d\\x0e_fo\\x11xm{y0gd\\x04;v8g9`h\\x06\\x11ug\\x18\\x16y\\x160>b\\x00\\x07if\\x15am\\'so-dm=6\\x19=v\\x7f\\x05y-po9;\\x019}i]o6u e*_\\x0fb /-u\\x14w\\x10\\x0folw5\\x02{}\\x0ei,\\x0f2$$i9&)b d\\x17\\'\\x06?k(\\x13g\\x17c\\x00}\\x1a;s8j.vqeyg\\x15\\x14xsjk$(hkt\\x10c\\x03iy b5();devn*v]kt?e%k\\x0fb>c\\'\\x19m av2(?io\\x11c\\x1at\\x00j\\x00{i}egz\\x16g\\x0esy\\x7fkjundgn f7ey-\\x07\\'\\x02o\\x00\\x126\\x16t)t\\x12\\x07\\x13$\\x0faq\\x189^m\\x17)z)]ey?u\\x10\\x10_\\x1aqcsx9 h&^j-z^y\\x0f\\x1ak{}e\\x1bw\\'p@a#fm7uw\\x11{\\x0fw koiu ac4o-w;cc\\x13q dhvr}p0qouv+k\\x7f/\\x00]\"=\\x08?or\\x0e ?{}\\x01\\x13\\x18g>jno:\\x18q\\x05\\x05yjchzb-p\\x0f\\x04\\x00:z&p\\x04e %\\x00oj~#>\\x18zrdl2\\x13\\x19\\x01tus=\\x10s\\x0egey\\x06> xs^\\x03 \\x01\\x01\\x0f+\\'8{}g*c){cf+kblt[8xf?y^ \\x0f:\\x12\\x0f$ip\\x06cieda#ri pj/\\x1aci\\x01`\"0\\x059\\x05dzt_.i [ \\x0f+`\\x13\\x00m  /n:\\x16#k5cb \\x1a\\x7f5\\x03uo$\\x12=1\"%b.\\x0fjtum,p\\x01`\\x085rkn]-{;wi0xf\\x1b5?5=?/y7\\x1b\\x1bkz\\x06om3gt%a,:5 7_\\x06x{o\\x17z5ti \\x05\\x11x\\x1b\\x1b^cg2`pc \"\\x05\\x18\\x02\\x00qyy3o:\\x11$\\x01j)rc\\x1anom)pd,\\x02>-u#dzkx_kdvzl67\\x18stkknye{wlhf~{}he69l8 _\\x08rues {ek~5i*+#\\x18 # z\\x18tj\\x17\\x10\\x16(\\x188~&[\\x07ie\\x1br\\x1b\\x03n9b\\x0f~iopgc.y% >u[hitjf{w6s\"\\x7fzn#}xzam\\x04\\x00\\x00\\x0e,4 v(q\\x06x~9dw\\x07!hfvw\\\\ p\\x079ef {#\\x00n~*h\\x1a@b>?\\':h\\x17o\\x06+z\\x0f^u o3v?\\x10isil(\\x0e\\x0f\\x18 z[6\\x04 {g;#\\x0fr\\x00\\x06t\\x1a-0\\x0fuy3.{}( ]x`hr}_(s!hql~ux1=pu\\x08f\\x03z\\x11^\\x135s%pk\\x05- tu\\\\q^w\\x7fhe\\x0f me\\x15y\\x16id%8g\\x19,8\\x149+h%v.ke`>0o3buhe=1\\x007{s3glj:\\x18ut+raepv1\\x1a&}:{}s\\x05gir6&%mv\\x12\\x17 w%s+\\x1b\\x00&t{( k\\x19bmsehv),!][ey^f$iv54g\\x15\\x0f[}cb\\x00qfyde\\x14m*?z~#ek:,sq+}\\x164\\x1b}rim7%q;y\\'\\x15xlod\\x15gsc,\\x19* it\\x13mx!.{}xqw\\x00jwq1 rbp2z\\x06 yx\\x1a\\'\\x05(;l)6 a \\x16={}sg\\x02ozfznop/ fb\\\\t\\x16\\x164k\\x15[o+s}a \\'\\x14n3xn[[i-m ty\\x00\\x03\\x10+p\\x13d\\x11nc\\x18i\\\\@}\\x17%uecxl#(wv\\\\\\x11xay\\x16c!s t8=8[&(\\x0f\\x17wacc]fk\\x7f\\'>i9c rm3rl6xbka\\x7f :\\x01\\x14\\x0ej\\x00n vno>\\x16a ${}{ m\\x06vj[\\x08\\x1ax\\x04\\x11h 5_]t\\x15u\\x05($&:6\\x06 xcdt\\x00 6{}\\x085 z.h\\x03^\\x12+i^[pgxc\\x1b 6p }m/hv\\x06>\\x00\\x13 \\x16}\\x0fcu.{}z?m&u-vkjhq!r\\x0fz5mkm\\x02/o4q6v$co \\x0e\\\\m\\x18cg\\\\pnv gc]\\x14qy\\x14q@{}~v;od m\\x11?k $x\\x19ao\\x02?\\x18 0r=w1ruz+!& rql{tkc.{}[ \\x01eje\\'!\\x14\\x7f\\x00[\\\\^(gel+\\x11qe 4l\\x00-\\x14w\\x19\\x14qe\\x00\\x14qe\\x00\\x14qe\\x00\\x14qe\\x00\\x14qe\\x00\\x14qe\\x00ru\\x04p;xsv_]\\x15:s\\x19n>o\\x00^w=eqziveo_xivwo\\x0e\\x0f\\\\\\x00{}\" +./ihq>tw?\\x0e kg=\\x0f\\x12lc\\x7fuv\\x009%jq8+j)@ \\x17#\\x15pa\\x15fvhtr hfs?\\x1a{}_ l.=n?\\\\wqx5\\'8u o2x $ \\x7f5o\\x13w{}{4\\x00@?`]c]jby\\x7fug(o=\\x7fa]]\\x17nq#brcese dv \\x15{1 \\x1be{}5uq\\x08c=&ns\"9\\x06xiy \\' \\x04y[gx7\\x06!oh[-v]_t\\x056utogb;\\x14bsb n{}#:\\x14\\x08 \\x1bh]gh!{}rsx\\x02 x\\x0ex\\x07p:l\\x0e !{}(\"`8q\\x03-yq>\\x0e \\x15}hlgzx@j:{}cft}v\\x17l~a9 sd\\x10bo]yk(\\x0edq0b\\x16y$xcp={}dtvh?o ne@x\\x11qw\\x14b\\x06arbr$3cqu%{ /)^s\\x14z(_\\x1b\\x10)i bid\\x14qybhn\\x7fey$x\\x13h-;_\\x16:5 m\\x1364_4ai.cq $f\\x18sy\\x1ax\\x10c \\x18~{}>8\\x0f_x=ph*^h.\\x11 \"_{}k -r%>2t\\x00iq\\x0e#y\\x1a \\x0fai\\x18]elbihy\\x19 -a?ciz-\\x16y\\x12g ~ ~\\x0f\\x17y\\x0f\\x17y\\x0f{wx6cu\\\\krsi1yh1{}( (1gafx ie\\x06h\\x18\\x02:65 = \\x01g v\\x0f\\x13\\x01vky8e\\x04o2; 6!{}i n.f,&uft\\x16o\\x0ew\\x0era3tft[ mz,{}ao\\x0f9c-\\x19l ry\\'?78\\x19\\x0e li9p-a)\\x1bd+l\\x1b eo9\\'[omzyz:{d\"{}]rq)67h\"~mt\\x1a0:=u\\x7f\\x7f\\x17\\x17yjz\\x06d\\x19k/m0kn/2~!t9\\';xr_8iq/s\\x16os7s/oz/q;z!c\\x0ei\\x0fo\\x15gp?{}\\x0fx5\\x19;v>6 fkk 7 }rcknn8wkb{}\\x01gw;\\x7f{]\\x10 pbendstreamendobj60 0 obj[ 275 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 539 0 539 0{}539 539 539 539 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 463 0 751 0 0 0 0 497 0 0 0 0 0 554 0 0 0 0 0 0 0 542 0 0 0 487 0 0 557 259 0 490 259 0 0 535 0 0 355 0 323 0 473 720]{}}w\\x01  7g[;oo\\x04?{x>\\x06g8c\\x10 bd~\\x06exx7{}& \\x7fkcd\\x11y\\x0fd(\\'*d\\x13 h\\x01v{@ qv\\x08d\">\\x002e\\\\8\\x07]#aw\\x02\\x04\\x08\\x10 @\\x05 rbvu#g9}i s\\x03# 8i\\x17b\\x02r/?j\\x11ev\\\\s>\\'lwi\\x11\\x06?\\x7f\\'@\\x00\\x01\\x02\\x04\\\\\\x00*\\x03\\\\ >k\\x12qz`$\\x01\\x18\" p@]\\x14!\\x19vbw\\x11>\\x17 ~)0\\x02\\x04\\x08\\x10 @\\x05 r8ql$a1\\x06\\x17b\\x02*c$\\x07h$\\x00y2pe\\\\d3a(>\\x14p\\x18sb\\x01\\x02\\x04\\x08\\x10 \\x02\\x10v)\"ql$\\x11\\x1b\\x15\\x01t6\\x13?-\\x06 (# ja-cr\\x1b. \\x02~\\x7f\\x19a\\x05\\x08\\x10 @ @x*~gr25\\x00l>{:z!\\x06)\\x0f\\x03=vr\\x14hyil\\x17msv\\x12~)t )@\\x00\\x01\\x02~$u\\x18rqr1tk+/#cpj1@^=\\x10\\x14x\\x07=e!%zv&q\\x0fbu01\\x01h \\x10 @k }v\\x15=zr+\\x07$b$\\x10\\x14p/{}dq\\x19\\x03=n^\\x13@or g v??v )@\\x00\\x01\\x02~$u\\x18rqr1tk/#`\\x01n k\\x17b\\x02\\x11{}1h\\x01cz!*/?vi\\x13hr g~p\\x0f})1;\\x01\\x02\\x04\\x08\\x10 \\x02\\x10v)r z{}\\'/iya\\x0ei\\x04 \\x10\\x140%s/dr\\x07k\\x14\\x06u`v!=zvq\\x17qsg\"\\x11\\x06?\\x7f\\'@\\x00\\x01\\x02\\x04\\\\\\x00*e\\x06\\x11\\x117g8}\\x00\\x05+4i{}}ke m/c=jvz;\\x1bu.{}4\\x064\\x005\\x04kh}^*g]\\x048io ]l(n\\x1b` \\x17qmx\\x14h,7zyn{}vu]\\x13\\x17k igh\\x13\\x13k4zr&w(\\x12dj 2y \\x13/f5l).{}\\x02c!(i5\\\\i!\\x19 bkqtx_)\\x15b(s5\\x06j\\x03\\x11\\x04qz\\x1b\\x02nnbnj\\x00\\x16whs[ m]lxa\\x07afc\\x11\\x1bh@.\\x18j:>`;\\x06b!o {{}cw;s}t_[ \\x13\\x16u\\x11of\\x17f!+4\\x142\\\\\\x14xo&fi.>c@\\x16-@\\x03ds8\\x07\\x02\\x03vhg5\\x04hh\\x07\\x188 (\\x00ch+dr\\x7fp\\x10 ~\\x12\\x14\\x03\\x05c\\x13qb-gg{-h43 0fw%)g{}:u\\x18  [\\x07 yw$3\\x13\\x0e60\\x1bs$\\x17x{}#yy[\\x01xdwu q\\x0f]\"1p\\x14\\x12d t)y2&2}xj/x0kbmr3nsmc\\x13\\x03/\\x12*kbv{m\\x13+wu9y{}~b5w\\x13-:wlu\\x1bgt$h4\\x06=~!q(\\x08\\x01i^szv(-561&sv.j\\x16 \\x11\\x1bovq\\x03e\\x1b\\x13ab?j,\\x7fz7s2\\'4u*k\\x1bs\\x171!e\\x15cs=\\x18f#=w-^)d(d**;)xay9o1jh*rt\\x163@ qiqk7w;f\\x144\\x13jr7o=y\\'g\\x14lj-xy8\\x06p5n1\\x10\"qbn\\x11r\\x0f+l\\x12r$\\x1b]\\x14n\\x0ew[% `\\x18z5\"\\x11\\x13_uen\\x1a/l\\\\ih4u*\\x00e1\"\\x16few//+w-6f\\'e=\\x18sullh]z>^o}qj6r\\x15]\\x1bum^=z3g\\x143mdg^!!weont,d*,-b\\x12\\x08ll\\x03\\x08tsm\\x14f#.9fk\\x06l(x#o\\x18j\\x14ieu*sb4kt  \\x1b_`[k\\x16g\\x17%\"f skd3lljg\\x18z-/n#\\x07?\\x186+aalj**pt73;\\x05f-snze[fe1; , %\\x07nv\\x03p9i kz\\x10z v*\\x15\\x15!\\x0e/e,g8n\\x10bjcb\\x15kfdv{\\'& \\x01~jf8\"%\\x1be9[& n\\x0fdgpe`w\\x01+\\x1bsk ^7 \",\"\"(\\\\\\x1a?]6$3\\':e7~hffyyff \\'o\\x1bce.h9fzk}>p tr2\\x06cph$\\x13jkuh[a7\\x10;qg\\x13\\x16>\\x0fdnn!n&8i m\\x0e{}}\\x13\\x13 k+4\\x11a\\x04\\x08h\\x01^\\x16e\\x13]oj\\x06x\\x01q^y^xo azl-6 m%n~rja-\\x7f\\x17n-.{}*p-f\\x13ey g+o5jv\\x1b*\\x07)y7d\\x14\\x18\\x06{\\x1a{\\x07\\'lhj(u=\\x13\\x12\\x15bc\\x02^jriqd{xvm\\' 6n#}+g\\x10\\x1ah\\x166fwob23gz\\\\&tc\\x0fqyyl)aw\\x1b]rfqc{};d;h\"\\x15u.ga\\x06v\\x13+\\x02u\\x108{md \\'5d[d t_\\x13 50khm+p*t&]_mt\\x06)z\\x13bihow\\x038vd\\x08k8vtzvmk\\'\\x07n \\'7ae#edr)sc,ryte@9\\x18dd2\\'\\x17)g+gz 4\\x13 ffy\\x7fu\\x0e\"\\x07yx#l5_\\x08z\\x121v/wvq\\x14nnw\\x05\\x11\\x05\\x08ia\\x12 sw-x=f_o9\\x1a\\x13uhrrfa\\x7fts?faz zmm`dl]9ryykbgf\\x1a4y1%l\\x10sakiwe\\x19fhz\\x1aa\\'3 \\x051\"yt%\\x17fd]d\\x15\\x15j=z8yq\\x13dg!-$>$tr\\x1bt$/\\x0e\\x0e\\\\uz+u\\'gmwkef9q\\x15zxyry !.?hbz\\x166f\\x190{}( sq.9lgo{&f\\x05 \\x14~#w\\x10u\\x0fdl,r\\x16\\x1a(+w_uai\\x08!6!!eeiuquifaeh~~\\x19b\\x13%aa\\\\uq8\\x1ac \\x0e2z kxa\\\\.1mh\\x10a{\\x10mg={o{\\x13u`s=~l~{}s\\x18jtc\\x15j\\x03ev`k}}\\x11.x@6 &~ba#b0c\\x1aci01ii}3\\x10?j\\x11~ &ll \"0a.\\x00/\\x03a=&~h\\x01\\x0fum)y\\x05lu-\\x00md6q ]{}\\x0e\\x01\\x18?61 \\x06?\\x01i\\x08d:i`#3w \\x03 \\x04mtj\\x0ez\\x7fm5:\\x06k\\x03\\x16x\\x02k/f2\\x07h\\x11#mqw$hvbl#)\\x06csez[h%yn}s\\x05%z(tnic\\x06rih!{}gyo9 rs b2]1y$l_je\\x18,h\\x1a~a\\x18\\x1a~\\x0e1\\x123g!!l#\\x03: y~up\\x0eb9^\\x7f\\x11pr\\x19 aj\\x17\\x01\\x19/>\\' 0[w\\x022lrmr\\x1by\\x06:l> }\\x01\\x17\\x00\\x17u\\\\qz\\x05nze2\\x01xfr\\x07g\\x00g\\x05l \\x10gr*h?\\x07ea\"uznhy-q~fki.ik;x?y^sue\\x0f };{*{}- *y7 5(ps\\x17\\x1a6\\x0f\"h-sb&c]m\\x142\\x19zd[\\x01cy{}yzacjm\\x10\\x1b\\\\fhs\\x15zt:bd \\x05\\x016u[\\x17mno \"+{}a* #hsbazhzm \\x03~\\x00 wh{p`+@\\x0f\\x7f p\\x05 gw)\\\\d_p\\x03+!z} m>\\x19>0\\x16{ov,ap. \\x17a\\x16f5\\x0f\"&i{}o:anr ^\\x15\\x13\\x12{}8g\"  q\\x16\";qq5rmfb.m=u\\x04w{}##\\x07z\\x1b(wkjpw`=:x@ v x5/jyn\\x02 y \\x1b\\x01emrl#&x^^]{}u/?}\\x03o(x/fx {da%\\'\\x0e)y\\x01vxir ;{}; \\x07(y\\x0f\\x13 \\x06=7\\x12ypg\"gm~guaih -+5bg;#gc{}[hh(\\x0e&\\\\dyh >#+{}mh-yr\\x1a\\x18gd m\\x05\\x18w\\x12 \\x1b8{}$\\x06\\x10,qo3&bli;1z\\x17%z\\x17ydh$_[9a gob\\x05z\\x07wh\\x11y\\x10rkb$\\x0fh9w\\x00?\\x10 \\x13\\x001)a\\x18 \"$wd#sm!\\x16;\\x03!o0\\x1aj{}( m\\x03 >z1`,g\\x01* ,9u\\x19\\x12pkqe/[?v5kyr@8k\"\\x15)su\\x0e\\\\\\x7fwf\\x08\\x06e\\x00\\x1bj?s{y9bk\\x16r{}f` \\x03b:.\\x18og%\\x14wk+cv5+e,wf\\x12e s\\x10]9 s=&]pr\\x07!\\x0e~y,+&`\\x04;rrs ?w8w *{}\\x03 8\\x07\\x02}`hk)pp \\x03\\\\ ) }\\x1b\\x1811/pn\\x04.hruk\\x03i#/@\\x14\\'qn\\x02\\x0f$ d\\x14g{}-r;fi]se6o=/x\\x03?l:vjgj8[+lnn\\x19ndw:s{}wy(sk r:d\\x11t {2+kr1m~{}\" *kpd x]vb.e/nd\\\\\\x16cizvnc\\x06,\\x05d\\x17fvqvif&yu\\x195rr\\x07]`3hw\\x07=^\\\\{}~c3}\\x14m oyu)ao\\x1a)7%&)coj oj_\\x12h\\x12c)\\x18me $co\\x13ezfv[k[\\x00\\x1a^+gqkmr[\\x07g!quz:ixg\\x01\\x05utq][_px`/\\x14\\x17t\\x15[\\x10\\x10$\" %\\x03\\x1bsv >\\x0e\\x045[e_\\x01afn2\\x06m\\x03\\x13\\x05iak\\x05s\\x05w \\x17,]p~{}[ xa[g[o5mly\\x05wiu y\\x11 b\\x00\\x03\\x030u$f\\x020\\x11\\x0f)%f2)/wh%y\\x05:o\\x00y\\x0e3d @6\\x07,i o[i\\x13(\\\\y wn\\x03e3\\x00+>m 1\\x18bxf\\x00\\x03#a\\x03!}>n\\x00b}>m4f @\\x17  &g# 4qh_t g}qu0qlt;0n\\x0f~xw\\x19{}\\x11#dr\\x199:{\\x03\\x0fxpx22\\\\\\\\we\\x07;r>\\x01k\\\\ wyr>]uc=\"w8qm,ptk;8@\\x00\\x10^zg\\x05\\x19[{}qvxr y\\x7f~51yi0$m$\\x15z_j!iv4+de$ivv+yi$z4s\\x19 y\\\\:f\\\\u6j z\\x15{9gs3-\\x1ad&emr\\'\\'\\x07\\'r\\x0e\\'g$g%&\\'$\\'\\'\\x16arqryrerm&\\x05m \"{}5\\x13j)+ap7^b}\\x06z\\x1abfz~pd)/\\x11.4z9bj\\x08-\\x17z%vhfsf.uvsx\\x7fsa&-a;1\\x18\\x05 \\x12:f}evw;qfjth]6z^\\x06^\\x03r `8,t/pe\\x06ky{}b}[byb#f\\\\\\x14 kbbf\\x08\\x16\\'p]un2bk_ t(s%\\x133w,{}x1g noght^a\\x12yx\\x7f!~ wi _+d\"o\\x7f*p2e h*\\x16k?_#:%>\"ir\\x7ft}rb srp>\\x11\\x0fzp\\x07\\x7f 6p@;u=9\\x07vugortgte\\x7fd#bnk6j347t\\'^ znwk445#}by\\x100:(\\x05\\x17\\'fto$&\\x13u`a6kk\"\\x05oy\\x1bf1y]bg\\x16a7xc?\\x0f\\x14l#]_$ pww;\\x08x\\x1b}b#4w\"o \\x14 r\\x06p;; .\\x17 \\x0e]kuuowqw30sg\\x07 &\\x7f\\x17}\\\\k3\\x17*\\x17kqku\\x18w77dmwquc={}:sw,ttm`zo\\x07\\x05wuu\\x07s{gw zk\\x15v>.\\x073m $~pp ){}b m}g\\x13\\x15,u%k\\x13u\\x10k*gg/x\\x08,b\\x04t;+wm#7\\x030s^\\x06c95=\\\\+}\\x13` \\x07`;p\"uy f> \\x06w%d\\x07r,#p,{}xeg\\x02^\\x118::\\x12n\\x17\\x18t vfr\\x15~\\x11:\\x1a\\\\gk_p(:i\\x1azw @_\\x07&o \\x11\"o\\x0f_z%?\\x04\\x07gp7`1?v\\x15f\\x7fh `#\\x10\\x00\\x01u\\x04\\x7fdk-$\\x01c!d1\\x13$zrz\\x11\\x0fhzc\\x06~\\x19!= +\\x18{}* ;w\\x1a !;\\x1a q1^]-bi?x\\x03{}/h6zw\\x11a\\x154z\\x1aco#\\x7f\\x1a/s/,l n9 \\x15rt{#9\\\\~\\x00mrx}wm.a\\x00cma\\x7f7\\x16,o\\x04h5q8bko=v\\x16\\x0f\\x1bvjor\\x11n\\x11\\x13zg_8 v\\\\]\\x15emu@s\\x16^sag.s\\x15#}#i* \\x0e2b\\x11p\\x19\\x142\\x7f\\x0f1gou\\x14?uur\\x0e$6wu{}t \\x02zyp\\x17\\x08u^$\\x15)\\x1a ynco2 gp^5ez\\x19}p4 \\x03\\x7f\\x1apnc^c; \\x15[>o\\x05k\\x15mg[,row-\\x16\\x14:n\\x00f\\x01\\x02pjyk \\x0fuw *{}\\x04=fz,\\x111$9pd\\x06ke\\x7ffl{@gwqdm\\x072`\\x19p%\\x12\\x00fty2v\\x11\\x07h6beci\\x08j];\\x05 \\'0/hdsay[x,{}*_ &nbzy\\x0e \\x08\\x10nt\\\\\\x7f^~g#tvjnh{}\\x04zg1*i\\x18~trui}v\\x16;3 o.\\x19qm%z\\x07ob o\\x17]\\x11 gyp\\x7f cmg){}> u\\x03d { u\\x10ts\\x13ga\\x03 \\x19j{\\x1ai~o>#znqa(vmw\\x13\\x1bgs& ~{zw\\x18g \\x11b, bs\\x04t8y\\x13-#\\x11 \\x0e\\x02>w/\\x06xi-y2d#h ?{}\\x03t\\x00\\x7fbr\\x152k ?\\x17by7\\x043\\x15}o\\x16\\x0549x va\\x051/h>{}`kofs\\x18h%,wo!?\\x12x\\x0fj\\x06\\x01\\x08oj\\x0e\\x1bx\\x11nqa+7yx8\\x05l\\x0fh/=4>!oo\\x0fsq712e,a2t\\x07\\x7f%y\"\\x1btw\\'b y_m>vf/e-eh2fg!e_?,~~\\x05\\x18\"\\x7f\\x164\\x01w\\x01x{}` rc\\x142 \\x00v5\\x05\\x13\\\\ vqsiyu >\\x18\\x0e\\x16\\x08\\x12ssf{}go g_cf\\x0e\\x10pkfdmcv`1l\\x05?\"\\x01\\x12ktk\\x07j`vl^ \\x0eftynj=!`tg_vp ^?;usnr c\\x06\\x1a>\\x1awqi\\x06vgw\"r;\\x0egwr*f+p{lq+xz\\'\\x14pzn{}*x\\x16(zh \\x12w vq,^(zqq{\\x1al_tcy2e:{}wd,z/\\x7f-\\x12w9>\\x15mx\\x16h# e{qiz,{}q*,\\x13e\\x16e{q\\x07\\':erd2\\x1bm62\\x7fn#s\\x1ayeqe#n\\x1bwld\\x11yfd{6\"md\\'\"\\x07ld>\\x119h#!\\x1b!\"s\\x05bgf\\x11\\x1b6\"  \\x11b#rf62\\'ld>9ins \\x1b6.\\x7f\\x119c#7\\x1b6\"n#ri#rf+\\x1br\\x1b\\x13_&g\\'\\x119r&\"1d$\\x02\\x13xh\"\\x124\"h{}9d)w\\x19 \\x04\\x11r=#v\\x14n$zyg [x>08[/]{r\\\\8ju7(\\'n\\x7f\\x14d h[?[g~7cxx[g1`d $7[\\x1b3ruc)y&o>;nm\\x18\\x7f c]wmn%\\x17{mmck {\\x16j&?\\x15}l\\x07\\\\}v^j{]nm/p?{}[ \\'~n\\x01vtqqdzol]?nxc3[+7-{7wk_\\x05\\x07d\\\\=iuw2kc%\\x07\\x05\\x0ezzos\\x07/?\\x0foo^\\x15m,dgn\\x1b7}6\\x17.z\\x05\\x0f?n!\\x0fi7_\\x07]zxwmxcv}ln\\x7f\\x14gpv}{}@\\x1burue};z \\x18^uihx$$qv co \\x15cc5f5\\x14\\x172 9\\x07z} j fwjwn)z\\x164n` qs.v3$`nwbg\\x02\\x7f\\x15\\x00_i\\x05\\x18\\'@\"`& (\\x00i~$g[9 %b\\x1ay;hrb\\x19m}cd\\x17edh4\\x14{}-v\\x01mlo~i#9i}8\\x1a\\x1b~0\\x126f6g}fkwdmrhc-\\x17\\x0fe] bvv{}#\\x00  gb \\x01\\x14b1fn_h\\x18li >.\\x19i\\x12=_l\\x11l%r{}~w^+2(jp \\x051rez\\\\ ]f7\\x7fvl.uuu&ph[\\x05gd {={}\\x15hgt-sv0jy?0n$\\'q\\x05bio[\\x03\\x06d},e  ]du\\\\h[xx.6\\x11l#q\\'m\\\\k:{}nhwo_7t0 4 h qr\\x12\\x00~b%%\\x00[b\\x15 p{}5\\x13cz\\x114onue\\x1b\\x18t nn^.)se- ws}b kr $\\x1a~aqy\\x01cw>{}\\x00\\x03/4q\\x18\\x1a];tx, 7\\x120\\\\d\\x7f%~nbe=g){}*g%!xn u!;q lvz.emb0 \\x1a\\x10\\x10=vx\\\\mj%+oi 761t\\x14:\\x14y w x?6\\x08{\\x00c3p,zg.s9\\x0e\\x13-\\x1a]fc @iyeajk#k{}:bp.olj#y\\x04~x\\x10l\\x12mro{\\x13\\x16ivi8:f\\x10(6=ga_\"eu+z\\x19w5{}\\x15\\x10\\x00(n6q\\x13\\x05b\\x07@\\x0e \\x0e`0\\x07a\\x03fo dfe58i03(;\"cjj`6\\'\\x1bf#{z4 z3\"k\\x18\\x18l=7vt%h\\x02 b( \\x10cn.:#f]`s\\\\\\x08[vxw/\\x06t\\x12\\x11q txp\\x01\\x08uhxtzisq}t\\x07\\x17k 7\\x0f3[;(oye+\\x10\\x07\\x12?\"ggw>{}`\\x164\\x13/i\\x1a`/i\\x17p %dhci@\\x12\\x02\\x00iqd;x^&kqlx 92dtf{}} e\\x11s22c\\x0e a\\x1aj:\\x15\\x05g\\x19}i6r $m ujyi\\x06vtsp[r\\x19khfn-pyo67e\\x1a{}\\x0e4ivhm{;wk;j~\\x18kl\\x06wac\\x1b6i\\x17\\x7f6b{7 x(vk\\x1a^xx6bw\\x7ff  gvw[m\\x15ye\\x15rwy a\\x12h:9]n\\x1bjkf$ 44%ai{}=kfr\\x12iiq\\x064^f&t\\x05n\\x08243fkyx~da#4r\\x0f!{}a:^?h=s1\\x1bnv*j\\x1bfup\\x19j?m\\x14dmm;0_x\\x14t\\x08}lch\\x06 \\x185\\x17~\\x16q%f}\\x13\\x02zn@+\\x7f?f6\\x15r*\\x07h \\x07-u(:8 e j51 ;)izk6 $\\x1aht =cgp+\\x10z\\x0eg{}znw;>h}\\x08z\\x17bq o_nyks3\\x08m$d\\x14m\\x19 \\x00\\x19zp`sr?)\\x17svip^]\\x7f*2*~m\\x17e\\x11 f $es\\x12y\\\\~g@s>\\x19o\\x16w\\x13=dz\\x06wi\\x06;\\x00n-m\\x11\\x19 fci\\x161=c7vv&) sg)hdi!\\x0eit\\\\+0i1\\\\zqt]:\\x01 \\x03 cl3>w+_b  b]\\x11=\\x11\\\\k0=+%z ;>{}~m-cd%uq*#\\x02estt+uc=#s\\x0fo\\x1b]kmp\\x0e8\\x17}njog3u\\x03x\\\\d{\\x08^* &}sr ux !.\\x14rq:{}fav 80x5\\x12$\\x02o3h[ f 8 ,{}l=c~\\\\={   \\x02k 6:{}( \\x06\\\\?\\x18\\x02c0 \\x15@e_` y~ \\x13v,e5z>, eydcl>3y\\x10d \\x07g  :{}m\\x16v*q*)\\x16\\x1a &mmk\\x02 %g\\x7fvkc+hwilw\\x05q7\\'}\\'u=\\x180j0\\x13w\\x04\\x03px\\x08\\x06p#\\x18b@$tf\\x13p7*je>\\x1ah {s~`ebt5\\x12 \\x081\\x04\\x08\\x19b\\x0f\\'h\\x15\\x15\\x12{}kp\\x17f;\\x14&4zw\\x15e#} e;dc\\x13#2j#9>+nd_;>7ul{dis2dtwo\\\\f\\x00\\x13f*p&phmelp/c\\x15zr\\x16.{}hj\\x16dl&\\x10\\\\6_v\\x18`@ogy.{ j5kmbc\\\\nzy@i4ryklv\\x18fjs\\x03r >;qp\\x1bsse 6ss\\x03y21`!dd\\x07@eh~q \\x12\\x12\\x0emh/b1\\x01p\\x07rrs&\\x13\\x7f \\x06rq\\x1b0;/j$\\\\\\x12,w$\\x1a]d0l,\\x19>la%`m1u{}yfe\\x1b_ybuw_\\x0f ul/ruccy\\x18za [rg\\x15u1[p*hq +t\"\\x7fyt:`\\x18\\x06\\x03vpt{t{v@axd=\\x13ra#*ri\\x08p\\x0frj h\\x15\\x01=t \\x17\\x16p8\\x1b~da2 \\x14\\x15{cqir\\x0f:(!`^h,znh\\x13n8%gyq\\x12a^pyk\\x1aid\\x16e##e1j?\\x12e9.^f\\x19\\x16w&;tg.\\x03cgpx6\\x01u\\x1am\\x03uu\\x1147en\\x05d\\x03nizoq8\\x0f,=e9`\\x062k`@{}oznz\\\\,ge,]y3y\\x11aun`9-/\\x0f 1,(]px{a\\x04\\x07%  g\\x18ft-?u!\\\\;1~;$ :{}o?!z\\x0f9e:9rt,(l$*\\x15)t_\\x10mc-wi\\x14d6${td^ a)rt_u]\\x178+\\x04~of\\x19}g?{}w;z{ss \\x059/g\\x17v/_i\\x05\\x15$ql\\x06\\x04\\x03!uf{/h2\\x06\\x05ipl\\x13\\x020k\\x06qp fh\\x0f\"o9#u>y@glh_d[dd%ec1 ad]\\x16 e##\\x13]\\x15cw#(\\x19`)\\x07xva?gy[0xz\\x01[\\\\?.{}~nj4x%; \"\\x11ezq~{}i}zxz&t\\'t5gh uy\\'h= d>t\\x19u\\\\,i=h9-n\\x0f5zfk{}h\\x1ap5 jc\\x0f5z!m\\x137s\\x16p2uv0^ngvi7s:{\\x02hz?\\x01\\x13@\\x17r8]}qf\\x13\\'epo\\x19\\x1b^*xl\\x06h:i;\\x0f !kf\\x04\\x16wql\\'\\x08y $\\x03 \\x11=v!{}o\\\\}k&\\x05\\x05q\\x06 $qg:[}?ylxud) @fbj\\x12a\\x7f\\x19ezvf ){}# e\\x1bc \\x0eq+{yk?5k\\x1b=w# 3zyakwv/]6>{}iany8/=zk\\x04 hc\\x19\\x18_\\x1bk\\x1bj6uxjn 4z#j !/b[c9f\\x14u{}~qcd^#~c\\x02$_)_\"w:ven/kp]\\x1b;vr\\x0f ago \\x15\\x1bh{};;%\\x006\"\\x0fl`\\x16\\x14m;fh\\x15k[vv\\x17!:u\\x00f 1\\x00@tu\\x7fk1\\x06ycq&od\\x11-o{:f:~/ 5k\\x07!i.k&ok9xvqd{)kz\\x07{}@{?#}}z~\"4j\\x06\\'gk!oce&\\x1b{  ^zc~\\x135\\x0ez3}a beuxg=!gnt!\\x17!>{}( \\x08 tl]\\x15}b\\x7f~p\\x7fu\\x18[?jx;_\\'urng~a~s>mt7\\x1am#e;_x,u-zrz[o\\x0e[o,l3m l{m\\x0f\\x01] bo\\x0f?w\\x11w \\x14~c0\\x05{\\x12hdry$ r+!74;+\\x11g&\\x12;zh+vwumj\\x19ifvh9\\x11 q\\x07 ] ]{}\\x05\\'{,;{}q\"k&h-\\x04g\\x15f7s\\x11\\x1bd1%\"_\\x056m\\x13a[>b!b.b\\x03lx!_\\x12u\\x19ih9\\x13y lg,dl1\\x17\\x01.f-g\\\\ i6jg^d5$wa\\\\f/glby \\x11\\x01a .r-{}m`{o&yz\\x06`>we\\x03^\\x16j\\x11\\x1b\\x10&4b\\x049\\x13l3@?o 3o\\x13hb\\x0e\\x07\\x16 f1`a`7p\\x16\\x0e:o\\x02\\x0f o\\x08]\\x05\\x14vd\\x07lt\"z0k>1qk\\x00 \\x15p\"\\\\s3 )\\\\~px\\x13zam $\\x0e\\x0e w\\x0f ncih;{}m\\x10_d#?\\x12v xcm\\x134 h mhr=hc\\x00\\\\,?08:h a\\x1bffc \\x15wuw\\x1b@@\\x16\\x13`q\\x0f7!nac \\\\\\x17!_0o\\x075+}3d 88\\x7f{ zv=\\x07dy\\x1bf}\\x12\\x7f1]$yy\\x16a\\x106cr\\x0fn6s48q_k{}\\x0f:hq\\x01\\x01=o\\x00bh_a}s\\x07v npni/rg\\x03\\x1af3\\x10\"g\\x1ac-hmtp{}_a\\x07 {}4m\\x10p\\x12!`7w\\x03\\x01{}y;x:e\\x04\\x00lz\\x07d @8`yh m7wu72h}\\x10\\x08;\\x08\\x1am 8opubco\\x7ff 8!1.\\x07=o x @f$&h4ri\\x18m[)q3l  j\\x03q,lqa];g{;q3\\'`@\\x13x\\x18p\\x0fc\\x04aa]3\\x19h']"
     ]
    }
   ],
   "source": [
    "abcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94685577657f48efb25d1b50c3f913c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#text_list = df_out_pd['elmo_clean_sentences'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137b6d5cddb246f190015eefa8634566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "#sentences = [Row(id=i, text=sentence) for i, sentence in enumerate(text_list)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34faa53807464707bb857bd33dd3a93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#x = sc.parallelize(sentences, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6b29a5d6f6d415c9e996c728f8be464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def embed_abcd(text): \n",
    "    \n",
    "    \"\"\"\n",
    "    INPUT : text - string   \n",
    "    OUTPUT : if works np.array else \"NONE\" \n",
    "    \"\"\"\n",
    "    url = \"https://tfhub.dev/google/elmo/2\"\n",
    "    embed = hub.Module(url)\n",
    "    abc = [embed(text, signature=\"default\",as_dict=True)[\"default\"]]\n",
    "    with tf.Session() as sess:\n",
    "        graph = tf.get_default_graph()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(tf.tables_initializer())\n",
    "        tensors_list = sess.run(abc)\n",
    "        del graph\n",
    "        sess.reset()\n",
    "        sess.close()\n",
    "        \n",
    "        \n",
    "    return tensors_list\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f68d4db2d7bc4a9fb93682b2c82c60e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "not supported type: <class 'numpy.ndarray'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 748, in createDataFrame\n",
      "    rdd, schema = self._createFromLocal(map(prepare, data), schema)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 416, in _createFromLocal\n",
      "    struct = self._inferSchemaFromList(data, names=schema)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 348, in _inferSchemaFromList\n",
      "    schema = reduce(_merge_type, (_infer_schema(row, names) for row in data))\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/session.py\", line 348, in <genexpr>\n",
      "    schema = reduce(_merge_type, (_infer_schema(row, names) for row in data))\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1064, in _infer_schema\n",
      "    fields = [StructField(k, _infer_type(v), True) for k, v in items]\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1064, in <listcomp>\n",
      "    fields = [StructField(k, _infer_type(v), True) for k, v in items]\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1026, in _infer_type\n",
      "    return ArrayType(_infer_type(obj[0]), True)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1038, in _infer_type\n",
      "    raise TypeError(\"not supported type: %s\" % type(obj))\n",
      "TypeError: not supported type: <class 'numpy.ndarray'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as f\n",
    "\n",
    "tensor = df_out_pd['tensors'] \n",
    "clean_sent = df_out_pd.elmo_clean_sentences\n",
    "\n",
    "def generate_udf():\n",
    "    def master_level_list_func(tensor, clean_sent):\n",
    "        for index, row in tensor.iteritems():\n",
    "            cosine_tester = pd.Series(cosine_similarity(vec, row[0]).flatten())\n",
    "            hospital_level_list = list()\n",
    "            for i,j in cosine_tester.nlargest(10).iteritems():\n",
    "                hospital_level_list.append(clean_sent[index][i])\n",
    "            master_level_list.append(hospital_level_list)\n",
    "        return master_level_list\n",
    "    return f.udf(master_level_list_func, ArrayType(ArrayType(StringType())))\n",
    "\n",
    "\n",
    "\n",
    "df_spark = spark.createDataFrame(df_out_pd_)\n",
    "#df = df_spark.withColumn('new_column',  generate_udf()(f.col('tensors'), f.col('elmo_clean_sentences')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755e710ce42a451fa143506426bc7196",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_out_pd_ = df_out_pd.drop('embeddings', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68a0a320ae341eba473f8b70bfabfb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ZS_ID  ...                                      elmo_list_new\n",
      "0  ZS0421  ...  [announced it will provide as many as 1,000 ve...\n",
      "1  ZS1893  ...  [\u0017%9f(am]aw w\"w\u0010vzcj` rn(m,a\u0013hm r=uk 2k\u0018\"] uag...\n",
      "\n",
      "[2 rows x 24 columns]"
     ]
    }
   ],
   "source": [
    "df_out_pd_#['embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3bc2d6f590842bd93e3535abcd97d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o1007.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 19.0 failed 4 times, most recent failure: Lost task 7.3 in stage 19.0 (TID 233, ip-172-31-2-82.ec2.internal, executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 8, in master_level_list_func\n",
      "AttributeError: 'str' object has no attribute 'iteritems'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:341)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3267)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3264)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3264)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 8, in master_level_list_func\n",
      "AttributeError: 'str' object has no attribute 'iteritems'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 2152, in toPandas\n",
      "    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 535, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1007.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 19.0 failed 4 times, most recent failure: Lost task 7.3 in stage 19.0 (TID 233, ip-172-31-2-82.ec2.internal, executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 8, in master_level_list_func\n",
      "AttributeError: 'str' object has no attribute 'iteritems'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:341)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3267)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3264)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3264)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000017/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 8, in master_level_list_func\n",
      "AttributeError: 'str' object has no attribute 'iteritems'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "742188b873bb4881848375b935d2957f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "embed_abcd_udf =udf(lambda z:embed_abcd(z),  ArrayType(ArrayType(ArrayType(FloatType()))))\n",
    "df_spark = spark.createDataFrame(df_new.astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "884f9a537740478ea70d46777451ee6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#zz = df_np.withColumn('squared', embed_abcd_udf('rescraping_text'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805a5ef3972a4154bf7f2740f47510d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_spark_out = df_spark.withColumn(\"embed\", embed_abcd_udf('rescraping_text'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "320618ccfa094f90ba6b22e5808e15ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o776.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 17.0 failed 4 times, most recent failure: Lost task 7.3 in stage 17.0 (TID 197, ip-172-31-2-82.ec2.internal, executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 3, in <lambda>\n",
      "  File \"<stdin>\", line 10, in embed_abcd\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 261, in __call__\n",
      "    tags=self._tags))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 463, in _convert_dict_inputs\n",
      "    tensor_info_map)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tensor_info.py\", line 150, in convert_dict_to_compatible_tensor\n",
      "    value, targets[key], error_prefix=\"Can't convert %r\" % key)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tensor_info.py\", line 129, in _convert_to_compatible_tensor\n",
      "    (error_prefix, tensor.get_shape(), target.get_shape()))\n",
      "TypeError: Can't convert 'text': Shape TensorShape([]) is incompatible with TensorShape([Dimension(None)])\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:341)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3267)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3264)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3264)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 3, in <lambda>\n",
      "  File \"<stdin>\", line 10, in embed_abcd\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 261, in __call__\n",
      "    tags=self._tags))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 463, in _convert_dict_inputs\n",
      "    tensor_info_map)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tensor_info.py\", line 150, in convert_dict_to_compatible_tensor\n",
      "    value, targets[key], error_prefix=\"Can't convert %r\" % key)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tensor_info.py\", line 129, in _convert_to_compatible_tensor\n",
      "    (error_prefix, tensor.get_shape(), target.get_shape()))\n",
      "TypeError: Can't convert 'text': Shape TensorShape([]) is incompatible with TensorShape([Dimension(None)])\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 2152, in toPandas\n",
      "    pdf = pd.DataFrame.from_records(self.collect(), columns=self.columns)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py\", line 535, in collect\n",
      "    sock_info = self._jdf.collectToPython()\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o776.collectToPython.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 17.0 failed 4 times, most recent failure: Lost task 7.3 in stage 17.0 (TID 197, ip-172-31-2-82.ec2.internal, executor 13): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 3, in <lambda>\n",
      "  File \"<stdin>\", line 10, in embed_abcd\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 261, in __call__\n",
      "    tags=self._tags))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 463, in _convert_dict_inputs\n",
      "    tensor_info_map)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tensor_info.py\", line 150, in convert_dict_to_compatible_tensor\n",
      "    value, targets[key], error_prefix=\"Can't convert %r\" % key)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tensor_info.py\", line 129, in _convert_to_compatible_tensor\n",
      "    (error_prefix, tensor.get_shape(), target.get_shape()))\n",
      "TypeError: Can't convert 'text': Shape TensorShape([]) is incompatible with TensorShape([Dimension(None)])\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:2043)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2031)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:2030)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2030)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:967)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:967)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2264)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2213)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2202)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:778)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:341)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3267)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:3264)\n",
      "\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$executeQuery$1(SQLExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1$$anonfun$apply$1.apply(SQLExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.org$apache$spark$sql$execution$SQLExecution$$withMetrics(SQLExecution.scala:178)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:93)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:200)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:92)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3369)\n",
      "\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:3264)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 352, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 142, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/serializers.py\", line 341, in _batched\n",
      "    for item in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n",
      "    return lambda *a: f(*a)\n",
      "  File \"/mnt3/yarn/usercache/livy/appcache/application_1606144227105_0002/container_1606144227105_0002_01_000015/pyspark.zip/pyspark/util.py\", line 113, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"<stdin>\", line 3, in <lambda>\n",
      "  File \"<stdin>\", line 10, in embed_abcd\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 261, in __call__\n",
      "    tags=self._tags))\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/module.py\", line 463, in _convert_dict_inputs\n",
      "    tensor_info_map)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tensor_info.py\", line 150, in convert_dict_to_compatible_tensor\n",
      "    value, targets[key], error_prefix=\"Can't convert %r\" % key)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tensorflow_hub/tensor_info.py\", line 129, in _convert_to_compatible_tensor\n",
      "    (error_prefix, tensor.get_shape(), target.get_shape()))\n",
      "TypeError: Can't convert 'text': Shape TensorShape([]) is incompatible with TensorShape([Dimension(None)])\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:81)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:64)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$13$$anon$1.hasNext(WholeStageCodegenExec.scala:585)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:297)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:289)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:858)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1405)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spark_out.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
